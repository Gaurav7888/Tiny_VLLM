# Tiny_vLLM


Tiny vLLM is an attempt to build an inference server from scratch, inspired by projects like **Tinygrad** and **TinyGPT**. With the growing need for efficient and lightweight inference solutions, Tiny vLLM aims to provide a minimal yet powerful foundation for serving large language models (LLMs) with maximum flexibility and control.

## ðŸš€ Inspiration

Over the past few years, large-scale language models have revolutionized AI applications. However, existing inference solutions often come with heavy dependencies, complex setups, and resource-intensive requirements. Projects like TinyTorch and TinyGPT demonstrated how minimalistic implementations could still be highly effective. Now, it's time to take on inference with **Tiny vLLM**â€”a lightweight yet capable inference server designed for maximum efficiency and simplicity.

## ðŸŽ¯ Goals

- **Minimalism** â€“ Strip down unnecessary complexity and provide a lightweight alternative to existing inference servers.
- **Performance** â€“ Optimize inference latency and memory usage while maintaining high throughput.
- **Flexibility** â€“ Make it easy to modify and extend for custom inference needs.
- **Simplicity** â€“ Provide a clean and understandable implementation that is easy to deploy and maintain.

## ðŸ”§ Features (Planned)

- Custom model loading and execution pipeline
- Efficient token generation and batching
- Dynamic quantization support for optimized performance
- REST API for serving LLM inference requests
- Lightweight and dependency-friendly design

## ðŸ’¡ Why Tiny vLLM?

While vLLM itself is a powerful and optimized inference engine, it can be overwhelming to integrate and modify. **Tiny vLLM** takes a fresh approach, allowing users to understand, customize, and deploy their own inference server without unnecessary overhead.

Stay tuned for updates as we build Tiny vLLM from the ground up! ðŸš€
